{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix for object detection training set synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist_trainset = datasets.MNIST(root='/Users/jcboyd/Data/torch', train=True, download=True)\n",
    "mnist_testset = datasets.MNIST(root='/Users/jcboyd/Data/torch', train=False, download=True)\n",
    "\n",
    "x_train = (mnist_trainset.data / 127.5) - 1\n",
    "x_test = (mnist_testset.data / 127.5) - 1\n",
    "\n",
    "y_train = mnist_trainset.targets\n",
    "y_test = mnist_testset.targets\n",
    "\n",
    "idx = (y_train == 0) | (y_train == 1)\n",
    "\n",
    "y_train = y_train[idx]\n",
    "x_train = x_train[idx]\n",
    "\n",
    "idx = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import UpsamplingNearest2d\n",
    "\n",
    "def get_canvas(x_data, y_data, num_samples=16, nb_classes=2, dim=256):\n",
    "\n",
    "    idx = torch.randint(x_data.shape[0], size=(num_samples,))\n",
    "    images = x_data[idx]\n",
    "    labels = y_data[idx]\n",
    "\n",
    "    h, w = images.shape[1:]\n",
    "\n",
    "    canvas = -torch.ones((dim, dim))\n",
    "    mask_img = -torch.ones((nb_classes, dim, dim))\n",
    "    bboxes = torch.Tensor()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        y, x = (torch.randint(dim - h, size=(1,)).item(), torch.randint(dim - w, size=(1,)).item())\n",
    "        canvas[y:y+h, x:x+w] = torch.max(canvas[y:y+h, x:x+w], images[i].squeeze())\n",
    "\n",
    "        s = 4\n",
    "\n",
    "        binary_noise = (torch.rand(h // s, w // s) > 0.5)[None, None].float()\n",
    "        binary_noise = (binary_noise - 0.5) / 0.5   # normalise to [-1, 1]\n",
    "        scaled_sample = UpsamplingNearest2d(scale_factor=(s, s))(binary_noise)\n",
    "\n",
    "        mask_img[labels[i], y:y+h, x:x+w] = scaled_sample.squeeze()\n",
    "        bboxes = torch.cat([bboxes, torch.tensor([[x, y, x + w, y + h]]).float()], axis=0)\n",
    "\n",
    "    canvas = torch.clamp(canvas, -1, 1)\n",
    "\n",
    "    return canvas[None, None], mask_img[None], bboxes\n",
    "\n",
    "\n",
    "def gen_canvas(x_data, y_data, batch_size=2):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        samples = [get_canvas(x_data, y_data) for _ in range(batch_size)]\n",
    "\n",
    "        canvas_batch = torch.cat([sample[0] for sample in samples])\n",
    "        mask_batch = torch.cat([sample[1] for sample in samples])\n",
    "\n",
    "        bbox_batch = torch.cat([torch.cat([i * torch.ones((16, 1)), sample[2]], axis=1)\n",
    "                               for i, sample in enumerate(samples)], axis=0)\n",
    "\n",
    "        yield mask_batch, canvas_batch, bbox_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = gen_canvas(x_train, y_train, batch_size=2)\n",
    "mask_batch, canvas_batch, bbox_batch = next(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15, 5), ncols=3)\n",
    "\n",
    "axes[0].imshow(canvas_batch[0].squeeze(), cmap='Greys_r')\n",
    "\n",
    "for i in range(1, 3):\n",
    "\n",
    "    axes[i].imshow(mask_batch[0, i - 1].squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Input\n",
    "# from keras.optimizers import Adam\n",
    "# from src.models import fnet, patch_gan\n",
    "# from src.utils import set_trainable\n",
    "\n",
    "# nb_classes = 2\n",
    "\n",
    "# input_shape = mask_batch.shape[1:]     # \"images\"\n",
    "# output_shape = canvas_batch.shape[1:]  # \"labels\"\n",
    "\n",
    "# h, w, c = output_shape\n",
    "# disc_patch = (h // 2 ** 4, w // 2 ** 4, c)\n",
    "\n",
    "# optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# # Build discriminator\n",
    "# discriminator = patch_gan(input_shape, output_shape)\n",
    "# discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Build the generator\n",
    "# generator = fnet(input_shape, 64, 'tanh')\n",
    "\n",
    "# # Input images and their conditioning images\n",
    "# images = Input(shape=input_shape)\n",
    "# labels = Input(shape=output_shape)\n",
    "\n",
    "# # By conditioning on B generate a fake version of A\n",
    "# fake_labels = generator(images)\n",
    "\n",
    "# set_trainable(discriminator, False)\n",
    "\n",
    "# # Discriminators determines validity of translated images / condition pairs\n",
    "# valid = discriminator([fake_labels, images])\n",
    "\n",
    "# combined = Model(inputs=[images, labels], outputs=[valid, fake_labels])\n",
    "# combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch_size = 2\n",
    "\n",
    "# # Adversarial loss ground truths\n",
    "# valid = np.ones((batch_size,) + disc_patch)\n",
    "# fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "# train_gen = gen_canvas(x_train, y_train, batch_size)\n",
    "# steps_per_epoch = 200 # x_train.shape[0] // batch_size // 4\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     for batch_i in range(steps_per_epoch):\n",
    "\n",
    "#         images, labels = next(train_gen)\n",
    "\n",
    "#         # Train discriminator - condition on B and generate a translated version\n",
    "#         fake_labels = generator.predict(images)\n",
    "\n",
    "#         set_trainable(discriminator, True)\n",
    "\n",
    "#         # Train the discriminators (original images = real / generated = Fake)\n",
    "#         d_loss_real = discriminator.train_on_batch([images, labels], valid)\n",
    "#         d_loss_fake = discriminator.train_on_batch([images, fake_labels], fake)\n",
    "#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "#         set_trainable(discriminator, False)\n",
    "\n",
    "#         # Train the generators\n",
    "#         g_loss = combined.train_on_batch([images, labels], [valid, labels])\n",
    "\n",
    "#         # Plot the progress\n",
    "#         print('[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f]' % (\n",
    "#             epoch, epochs, batch_i, steps_per_epoch, d_loss[0], 100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "#     generator.save_weights('./weights/pix2pix_mnist.h5')\n",
    "#     discriminator.save_weights('./weights/patch_gan_mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights('./weights/pix2pix_mnist_199.h5')\n",
    "# discriminator.load_weights('./weights/patch_gan_mnist_199.h5')\n",
    "\n",
    "# gen_test = gen_canvas(x_test, y_test, 3)\n",
    "# images, labels, bboxes, roi_labels = next(gen_test)\n",
    "\n",
    "# fake_labels = generator.predict((images + 1) / 2)\n",
    "\n",
    "# titles = ['Original', 'Generated']\n",
    "# fig, axes = plt.subplots(figsize=(10, 15), nrows=3, ncols=2)\n",
    "\n",
    "# axes[0, 0].set_title('Original')\n",
    "# axes[0, 1].set_title('Generated')\n",
    "\n",
    "# for i in range(3):\n",
    "\n",
    "#     axes[i, 0].imshow(labels[i].squeeze())\n",
    "#     axes[i, 1].imshow(fake_labels[i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(figsize=(10, 5), nrows=4, ncols=4)\n",
    "\n",
    "# img = fake_labels[0, ..., 0]\n",
    "\n",
    "# for i in range(16):\n",
    "\n",
    "#     ax = axes[i // 4][i % 4]\n",
    "\n",
    "#     left, top, right, bottom = list(map(int, bboxes[i, 1:]))\n",
    "#     ax.imshow(img[top:bottom, left:right])#[220:250, 170:200])\n",
    "#     ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant degree of mode collapse in the generated objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainable(model, trainable):\n",
    "\n",
    "    for param in model.parameters():\n",
    "\n",
    "        param.requires_grad = trainable\n",
    "\n",
    "#         if not trainable:\n",
    "#             param.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Linear, Conv2d, ReLU, LeakyReLU, BatchNorm1d, BatchNorm2d, Tanh\n",
    "from torch.nn import MaxPool2d, UpsamplingNearest2d, ZeroPad2d\n",
    "\n",
    "from torchvision.ops import RoIAlign, RoIPool\n",
    "\n",
    "\n",
    "class Downward(Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, normalise=True):\n",
    "\n",
    "        super(Downward, self).__init__()\n",
    "\n",
    "        self.pool = Sequential(\n",
    "            ZeroPad2d((1, 2, 1, 2)),\n",
    "            Conv2d(in_ch, out_ch, kernel_size=4, stride=2),\n",
    "            LeakyReLU(negative_slope=0.2, inplace=True))\n",
    "\n",
    "        if normalise:\n",
    "            self.pool.add_module('batch_norm', BatchNorm2d(out_ch, momentum=0.8))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upward(Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "\n",
    "        super(Upward, self).__init__()\n",
    "\n",
    "        self.depool = Sequential(\n",
    "            UpsamplingNearest2d(scale_factor=2),\n",
    "            ZeroPad2d((1, 2, 1, 2)),\n",
    "            Conv2d(in_ch, out_ch, kernel_size=4, stride=1),\n",
    "            ReLU(inplace=True),\n",
    "            BatchNorm2d(out_ch, momentum=0.8))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = self.depool(x1)\n",
    "        x = torch.cat((x, x2), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class FNet(Module):\n",
    "\n",
    "    def __init__(self, base_filters=32):\n",
    "\n",
    "        super(FNet, self).__init__()\n",
    "\n",
    "        self.down1 = Downward(2, base_filters, normalise=False)\n",
    "        self.down2 = Downward(base_filters, 2 * base_filters)\n",
    "        self.down3 = Downward(2 * base_filters, 4 * base_filters)\n",
    "        self.down4 = Downward(4 * base_filters, 8 * base_filters)\n",
    "        self.down5 = Downward(8 * base_filters, 8 * base_filters)\n",
    "#         self.down6 = Downward(8 * base_filters, 8 * base_filters)\n",
    "#         self.down7 = Downward(8 * base_filters, 8 * base_filters)\n",
    "\n",
    "        self.up1 = Upward(8 * base_filters, 8 * base_filters)\n",
    "        self.up2 = Upward(16 * base_filters, 4 * base_filters)\n",
    "        self.up3 = Upward(8 * base_filters, 2 * base_filters)\n",
    "        self.up4 = Upward(4 * base_filters, base_filters)\n",
    "\n",
    "#         self.up1 = Upward(8 * base_filters, 8 * base_filters)\n",
    "#         self.up2 = Upward(16 * base_filters, 8 * base_filters)\n",
    "#         self.up3 = Upward(16 * base_filters, 8 * base_filters)\n",
    "#         self.up4 = Upward(16 * base_filters, 4 * base_filters)\n",
    "#         self.up5 = Upward(8 * base_filters, 2 * base_filters)\n",
    "#         self.up6 = Upward(4 * base_filters, base_filters)\n",
    "\n",
    "        self.out_conv = Sequential(\n",
    "            UpsamplingNearest2d(scale_factor=2),\n",
    "            ZeroPad2d((1, 2, 1, 2)),\n",
    "            Conv2d(2 * base_filters, 1, kernel_size=4, stride=1),\n",
    "            Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(x1)\n",
    "        x3 = self.down3(x2)\n",
    "        x4 = self.down4(x3)\n",
    "        x5 = self.down5(x4)\n",
    "#         x6 = self.down6(x5)\n",
    "#         x7 = self.down7(x6)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "#         x = self.up1(x7, x6)\n",
    "#         x = self.up2(x, x5)\n",
    "#         x = self.up3(x, x4)\n",
    "#         x = self.up4(x, x3)\n",
    "#         x = self.up5(x, x2)\n",
    "#         x = self.up6(x, x1)\n",
    "\n",
    "        x = self.out_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchGAN(Module):\n",
    "\n",
    "    def __init__(self, base_filters=32):\n",
    "\n",
    "        super(PatchGAN, self).__init__()\n",
    "\n",
    "        self.down1 = Downward(3, base_filters, normalise=False)\n",
    "        self.down2 = Downward(base_filters, 2 * base_filters)\n",
    "        self.down3 = Downward(2 * base_filters, 4 * base_filters)\n",
    "        self.down4 = Downward(4 * base_filters, 8 * base_filters)\n",
    "\n",
    "        self.padding = ZeroPad2d((1, 2, 1, 2))\n",
    "        self.validity = Conv2d(8 * base_filters, 1, kernel_size=4, stride=1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "\n",
    "        x = self.down1(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.down3(x)\n",
    "        x = self.down4(x)\n",
    "\n",
    "        x = self.padding(x)\n",
    "        x = self.validity(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class RoIGAN(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(RoIGAN, self).__init__()\n",
    "\n",
    "        self.features = Sequential(\n",
    "            Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            BatchNorm2d(16, momentum=0.8),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            BatchNorm2d(32, momentum=0.8),\n",
    "            Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "            BatchNorm2d(32, momentum=0.8))\n",
    "\n",
    "#         self.roi_align = RoIAlign(output_size=(7, 7),\n",
    "#                                   spatial_scale=1, sampling_ratio=-1)\n",
    "        self.roi_align = RoIPool(output_size=(7, 7), spatial_scale=1)\n",
    "\n",
    "        self.classifier = Sequential(\n",
    "            Linear(1568, 1))\n",
    "\n",
    "    def forward(self, x, y, boxes):\n",
    "\n",
    "        x = torch.cat([x, y], axis=1)\n",
    "\n",
    "        features = self.features(x)\n",
    "        # N.B. box coordinates correspond to features, not input_img\n",
    "        roi = self.roi_align(features, boxes)\n",
    "\n",
    "        roi_flat = roi.view(-1, 1568)\n",
    "        valid = self.classifier(roi_flat)\n",
    "\n",
    "        return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FNet().to(device)\n",
    "discriminator = PatchGAN().to(device)\n",
    "roi_gan = RoIGAN().to(device)\n",
    "\n",
    "total_params = 0\n",
    "\n",
    "for params in generator.parameters():\n",
    "    total_params += torch.prod(torch.tensor(params.size())).item()\n",
    "\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss, L1Loss, BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_gen = gen_canvas(x_train, y_train, batch_size)\n",
    "mask_batch, canvas_batch, bbox_batch = next(train_gen)\n",
    "\n",
    "c, h, w = mask_batch.shape[1:]\n",
    "\n",
    "disc_patch = (1, h // 2 ** 4, w // 2 ** 4)\n",
    "\n",
    "# Loss weights\n",
    "lambda_recon = 100\n",
    "\n",
    "optimiser_discriminator = Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimiser_roi = Adam(roi_gan.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimiser_combined = Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "steps_per_epoch = 10 # 200\n",
    "\n",
    "\n",
    "def train_on_batch_discriminator(discriminator, imgs, optimiser):\n",
    "\n",
    "    images, labels, fake_labels = imgs\n",
    "\n",
    "    input_images = torch.cat([images, images], axis=0)\n",
    "    input_labels = torch.cat([labels, fake_labels], axis=0)\n",
    "\n",
    "    valid = torch.ones((batch_size,) + disc_patch)\n",
    "    fake = torch.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "    targets = torch.cat([valid, fake], axis=0).to(device)\n",
    "\n",
    "    outputs = discriminator(input_images, input_labels)\n",
    "\n",
    "    # clear previous gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    d_loss = MSELoss()(outputs, targets)\n",
    "\n",
    "    # calculate gradients\n",
    "    d_loss.backward()\n",
    "\n",
    "    # descent step\n",
    "    optimiser.step()\n",
    "\n",
    "    return d_loss\n",
    "\n",
    "\n",
    "def train_on_batch_roi_gan(roi_gan, data, optimiser):\n",
    "\n",
    "    images, labels, fake_labels, bboxes = data\n",
    "\n",
    "    num_roi = bboxes.shape[0]\n",
    "\n",
    "    input_images = torch.cat([images, images], axis=0)\n",
    "    input_labels = torch.cat([labels, fake_labels], axis=0)\n",
    "    input_bboxes = torch.cat([bboxes, bboxes], axis=0)\n",
    "\n",
    "    input_bboxes[num_roi:, 0] += batch_size  # increment image id\n",
    "    input_bboxes[:, 1:] = input_bboxes[:, 1:] / 2  # correct for pooling\n",
    "\n",
    "#     valid = 0.7 + 0.5 * torch.rand((num_roi, 1))\n",
    "#     fake = 0.3 * torch.rand((num_roi, 1))\n",
    "\n",
    "    valid = torch.ones((num_roi, 1))\n",
    "    fake = torch.zeros((num_roi, 1))\n",
    "\n",
    "    targets = torch.cat([valid, fake], axis=0).to(device)\n",
    "\n",
    "    # clear previous gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # forward pass\n",
    "    validity = roi_gan(input_images, input_labels, input_bboxes)\n",
    "\n",
    "    # calculate loss\n",
    "    d_loss = MSELoss()(validity, targets)\n",
    "\n",
    "    # backpropagate\n",
    "    d_loss.backward()\n",
    "\n",
    "    # descent step\n",
    "    optimiser.step()\n",
    "\n",
    "    return d_loss\n",
    "\n",
    "\n",
    "def train_on_batch_combined(models, data, optimiser):\n",
    "\n",
    "    # clear previous gradients\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    generator, discriminator, roi_gan = models\n",
    "    images, labels, bboxes = data\n",
    "    bboxes[:, 1:] = bboxes[:, 1:] / 2  # correct for pooling\n",
    "\n",
    "    fake_labels = generator(images)\n",
    "\n",
    "    # Discriminators determines validity of translated images\n",
    "    valid = torch.ones((batch_size,) + disc_patch).to(device)\n",
    "    validity = discriminator(images, fake_labels)\n",
    "\n",
    "    # Discriminators determines validity of translated images\n",
    "    num_roi = bboxes.shape[0]\n",
    "    valid_roi = torch.ones((num_roi, 1)).to(device)\n",
    "    validity_roi = roi_gan(images, fake_labels, bboxes)\n",
    "\n",
    "    g_loss = 0 * MSELoss()(validity, valid) + \\\n",
    "             1 * MSELoss()(validity_roi, valid_roi) + \\\n",
    "             10 * L1Loss()(labels, fake_labels)\n",
    "\n",
    "    # calculate gradients\n",
    "    g_loss.backward()\n",
    "\n",
    "    # descent step\n",
    "    optimiser.step()\n",
    "\n",
    "    return g_loss\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_i in range(steps_per_epoch):\n",
    "\n",
    "        data = next(train_gen)\n",
    "        images, labels, bboxes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_labels = generator(images).detach()\n",
    "\n",
    "        # Train discriminator\n",
    "        set_trainable(discriminator, True)\n",
    "\n",
    "        d_loss = train_on_batch_discriminator(discriminator,\n",
    "                                              [images, labels, fake_labels],\n",
    "                                              optimiser_discriminator).item()\n",
    "\n",
    "        set_trainable(discriminator, False)\n",
    "\n",
    "        # Train roi discriminator\n",
    "        set_trainable(roi_gan, True)\n",
    "\n",
    "        roi_loss = train_on_batch_roi_gan(roi_gan,\n",
    "                                          [images, labels, fake_labels, bboxes],\n",
    "                                          optimiser_roi).item()\n",
    "\n",
    "        set_trainable(roi_gan, False)\n",
    "\n",
    "        # Train combined pix2pix\n",
    "        g_loss = train_on_batch_combined([generator, discriminator, roi_gan],\n",
    "                                         [images, labels, bboxes],\n",
    "                                         optimiser_combined).item()\n",
    "\n",
    "        # Plot the progress\n",
    "        print('[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [R loss: %f] [G loss: %05f]' % (\n",
    "            epoch, epochs, batch_i, steps_per_epoch, d_loss, roi_loss, g_loss))\n",
    "\n",
    "    # save model weights\n",
    "    torch.save(generator.state_dict(), './pix2pix_generator_%d.torch' % epoch)\n",
    "    torch.save(discriminator.state_dict(), './pix2pix_discriminator_%d.torch' % epoch)\n",
    "    torch.save(roi_gan.state_dict(), './pix2pix_discriminator_%d.torch' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 199 \n",
    "generator = FNet().to(device)\n",
    "generator.load_state_dict(torch.load('./weights//pix2pix_generator_%d.torch' % epoch,\n",
    "                                     map_location=torch.device('cpu')))\n",
    "generator = generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise progress\n",
    "gen_test = gen_canvas(x_test, y_test, batch_size=3)\n",
    "data = next(gen_test)\n",
    "images, labels, bboxes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "\n",
    "fake_labels = generator(images)\n",
    "\n",
    "titles = ['Original', 'Generated']\n",
    "fig, axes = plt.subplots(figsize=(10, 15), nrows=3, ncols=2)\n",
    "\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 1].set_title('Generated')\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    axes[i, 0].imshow(labels.detach().cpu().numpy()[i].squeeze())\n",
    "    axes[i, 1].imshow(fake_labels.detach().cpu().numpy()[i].squeeze())\n",
    "\n",
    "# fig.savefig('./outputs/torch_%04d.png' % epoch)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
