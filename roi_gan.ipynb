{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix for object detection training set synthesis\n",
    "\n",
    "An example of how one can synthesise a multi-class training set for object detection using a pix2pix framework. We assume we have access to a library of annotated object crops (here, MNIST digits 0 and 1). We propose a discriminator with a RoIPool (of R-CNN fame), which we find to be a workable option, and superior in our experiments to the usual PatchGAN discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "\n",
    "mnist_trainset = datasets.MNIST (root='/Users/jcboyd/Data/torch', train=True, download=True)\n",
    "mnist_testset = datasets.MNIST(root='/Users/jcboyd/Data/torch', train=False, download=True)\n",
    "\n",
    "x_train = (mnist_trainset.data / 127.5) - 1\n",
    "x_test = (mnist_testset.data / 127.5) - 1\n",
    "\n",
    "y_train = mnist_trainset.targets\n",
    "y_test = mnist_testset.targets\n",
    "\n",
    "idx = (y_train == 0) | (y_train == 1)\n",
    "\n",
    "x_train = x_train[idx]\n",
    "y_train = y_train[idx] # - 4\n",
    "\n",
    "idx = (y_test == 0) | (y_test == 1)\n",
    "\n",
    "x_test = x_test[idx]\n",
    "y_test = y_test[idx] # - 4\n",
    "\n",
    "nb_classes = torch.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import UpsamplingNearest2d\n",
    "\n",
    "def get_canvas(x_data, y_data, num_samples=16, dim=256):\n",
    "\n",
    "    idx = torch.randint(x_data.shape[0], size=(num_samples,))\n",
    "    images = x_data[idx]\n",
    "    labels = y_data[idx]\n",
    "\n",
    "    h, w = images.shape[1:]\n",
    "\n",
    "    canvas = -torch.ones((dim, dim))\n",
    "    mask_img = -torch.ones((nb_classes, dim, dim))\n",
    "    bboxes = torch.Tensor()\n",
    "\n",
    "    for i in range(num_samples):\n",
    "\n",
    "        y, x = (torch.randint(dim - h, size=(1,)).item(), torch.randint(dim - w, size=(1,)).item())\n",
    "        canvas[y:y+h, x:x+w] = torch.max(canvas[y:y+h, x:x+w], images[i].squeeze())\n",
    "\n",
    "        s = 4\n",
    "\n",
    "        binary_noise = (torch.rand(h // s, w // s) > 0.5)[None, None].float()\n",
    "        binary_noise = (binary_noise - 0.5) / 0.5   # normalise to [-1, 1]\n",
    "        scaled_sample = UpsamplingNearest2d(scale_factor=(s, s))(binary_noise)\n",
    "\n",
    "        mask_img[labels[i], y:y+h, x:x+w] = scaled_sample.squeeze()\n",
    "        bboxes = torch.cat([bboxes, torch.tensor([[x, y, x + w, y + h]]).float()], axis=0)\n",
    "\n",
    "    canvas = torch.clamp(canvas, -1, 1)\n",
    "\n",
    "    return canvas[None, None], mask_img[None], bboxes\n",
    "\n",
    "\n",
    "def gen_canvas(x_data, y_data, batch_size=2, num_samples=16):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        samples = [get_canvas(x_data, y_data, num_samples) for _ in range(batch_size)]\n",
    "\n",
    "        canvas_batch = torch.cat([sample[0] for sample in samples])\n",
    "        mask_batch = torch.cat([sample[1] for sample in samples])\n",
    "\n",
    "        bbox_batch = torch.cat([torch.cat([i * torch.ones((num_samples, 1)), sample[2]], axis=1)\n",
    "                               for i, sample in enumerate(samples)], axis=0)\n",
    "\n",
    "        yield mask_batch, canvas_batch, bbox_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12fda6ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_gen = gen_canvas(x_train, y_train, batch_size=2)\n",
    "mask_batch, canvas_batch, bbox_batch = next(train_gen)\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(15, 5), ncols=3)\n",
    "\n",
    "for i in range(2):\n",
    "    axes[i].imshow(mask_batch[0, i - 1].squeeze())\n",
    "\n",
    "axes[2].imshow(canvas_batch[0].squeeze(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "# from keras.layers import Input\n",
    "# from keras.optimizers import Adam\n",
    "# from src.models import fnet, patch_gan\n",
    "# from src.utils import set_trainable\n",
    "\n",
    "# nb_classes = 2\n",
    "\n",
    "# input_shape = mask_batch.shape[1:]     # \"images\"\n",
    "# output_shape = canvas_batch.shape[1:]  # \"labels\"\n",
    "\n",
    "# h, w, c = output_shape\n",
    "# disc_patch = (h // 2 ** 4, w // 2 ** 4, c)\n",
    "\n",
    "# optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# # Build discriminator\n",
    "# discriminator = patch_gan(input_shape, output_shape)\n",
    "# discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# # Build the generator\n",
    "# generator = fnet(input_shape, 64, 'tanh')\n",
    "\n",
    "# # Input images and their conditioning images\n",
    "# images = Input(shape=input_shape)\n",
    "# labels = Input(shape=output_shape)\n",
    "\n",
    "# # By conditioning on B generate a fake version of A\n",
    "# fake_labels = generator(images)\n",
    "\n",
    "# set_trainable(discriminator, False)\n",
    "\n",
    "# # Discriminators determines validity of translated images / condition pairs\n",
    "# valid = discriminator([fake_labels, images])\n",
    "\n",
    "# combined = Model(inputs=[images, labels], outputs=[valid, fake_labels])\n",
    "# combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# batch_size = 2\n",
    "\n",
    "# # Adversarial loss ground truths\n",
    "# valid = np.ones((batch_size,) + disc_patch)\n",
    "# fake = np.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "# train_gen = gen_canvas(x_train, y_train, batch_size)\n",
    "# steps_per_epoch = 200 # x_train.shape[0] // batch_size // 4\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "\n",
    "#     for batch_i in range(steps_per_epoch):\n",
    "\n",
    "#         images, labels = next(train_gen)\n",
    "\n",
    "#         # Train discriminator - condition on B and generate a translated version\n",
    "#         fake_labels = generator.predict(images)\n",
    "\n",
    "#         set_trainable(discriminator, True)\n",
    "\n",
    "#         # Train the discriminators (original images = real / generated = Fake)\n",
    "#         d_loss_real = discriminator.train_on_batch([images, labels], valid)\n",
    "#         d_loss_fake = discriminator.train_on_batch([images, fake_labels], fake)\n",
    "#         d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "#         set_trainable(discriminator, False)\n",
    "\n",
    "#         # Train the generators\n",
    "#         g_loss = combined.train_on_batch([images, labels], [valid, labels])\n",
    "\n",
    "#         # Plot the progress\n",
    "#         print('[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f]' % (\n",
    "#             epoch, epochs, batch_i, steps_per_epoch, d_loss[0], 100 * d_loss[1], g_loss[0]))\n",
    "\n",
    "#     generator.save_weights('./weights/pix2pix_mnist.h5')\n",
    "#     discriminator.save_weights('./weights/patch_gan_mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.load_weights('./weights/pix2pix_mnist_199.h5')\n",
    "# discriminator.load_weights('./weights/patch_gan_mnist_199.h5')\n",
    "\n",
    "# gen_test = gen_canvas(x_test, y_test, 3)\n",
    "# images, labels, bboxes, roi_labels = next(gen_test)\n",
    "\n",
    "# fake_labels = generator.predict((images + 1) / 2)\n",
    "\n",
    "# titles = ['Original', 'Generated']\n",
    "# fig, axes = plt.subplots(figsize=(10, 15), nrows=3, ncols=2)\n",
    "\n",
    "# axes[0, 0].set_title('Original')\n",
    "# axes[0, 1].set_title('Generated')\n",
    "\n",
    "# for i in range(3):\n",
    "\n",
    "#     axes[i, 0].imshow(labels[i].squeeze())\n",
    "#     axes[i, 1].imshow(fake_labels[i].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(figsize=(10, 5), nrows=4, ncols=4)\n",
    "\n",
    "# img = fake_labels[0, ..., 0]\n",
    "\n",
    "# for i in range(16):\n",
    "\n",
    "#     ax = axes[i // 4][i % 4]\n",
    "\n",
    "#     left, top, right, bottom = list(map(int, bboxes[i, 1:]))\n",
    "#     ax.imshow(img[top:bottom, left:right])#[220:250, 170:200])\n",
    "#     ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant degree of mode collapse in the generated objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18385\n"
     ]
    }
   ],
   "source": [
    "from src.models import FNet, PatchGAN, RoIGAN\n",
    "\n",
    "generator = FNet(nb_classes).to(device)\n",
    "discriminator = PatchGAN(nb_classes,).to(device)\n",
    "roi_gan = RoIGAN(nb_classes,).to(device)\n",
    "\n",
    "total_params = 0\n",
    "\n",
    "for params in roi_gan.parameters():\n",
    "    total_params += torch.prod(torch.tensor(params.size())).item()\n",
    "\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-77961c026420>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    144\u001b[0m         d_loss = torch.sqrt(train_on_batch_discriminator(discriminator,\n\u001b[1;32m    145\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                             optimiser_discriminator)).item()\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mset_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/roi_gan/src/training.py\u001b[0m in \u001b[0;36mtrain_on_batch_discriminator\u001b[0;34m(discriminator, imgs, optimiser)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdisc_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# from torch.nn import MSELoss, L1Loss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from src.training import train_on_batch_discriminator\n",
    "from src.training import train_on_batch_roi_gan\n",
    "from src.training import train_on_batch_combined\n",
    "\n",
    "\n",
    "def set_trainable(model, trainable):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = trainable\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "train_gen = gen_canvas(x_train, y_train, batch_size)\n",
    "mask_batch, canvas_batch, bbox_batch = next(train_gen)\n",
    "\n",
    "c, h, w = mask_batch.shape[1:]\n",
    "\n",
    "disc_patch = (1, h // 2 ** 2, w // 2 ** 2)\n",
    "\n",
    "optimiser_discriminator = Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimiser_roi = Adam(roi_gan.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optimiser_combined = Adam(generator.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "epochs = 200\n",
    "steps_per_epoch = 20\n",
    "\n",
    "# def train_on_batch_discriminator(discriminator, imgs, optimiser):\n",
    "\n",
    "#     images, labels, fake_labels = imgs\n",
    "\n",
    "#     input_images = torch.cat([images, images], axis=0)\n",
    "#     input_labels = torch.cat([labels, fake_labels], axis=0)\n",
    "\n",
    "#     valid = torch.ones((batch_size,) + disc_patch)\n",
    "#     fake = torch.zeros((batch_size,) + disc_patch)\n",
    "\n",
    "#     targets = torch.cat([valid, fake], axis=0).to(device)\n",
    "\n",
    "#     outputs = discriminator(input_images, input_labels)\n",
    "\n",
    "#     # clear previous gradients\n",
    "#     optimiser.zero_grad()\n",
    "\n",
    "#     # forward pass\n",
    "#     d_loss = MSELoss()(outputs, targets)\n",
    "\n",
    "#     # calculate gradients\n",
    "#     d_loss.backward()\n",
    "\n",
    "#     # descent step\n",
    "#     optimiser.step()\n",
    "\n",
    "#     return d_loss\n",
    "\n",
    "\n",
    "# def train_on_batch_roi_gan(roi_gan, data, optimiser):\n",
    "\n",
    "#     images, labels, fake_labels, bboxes = data\n",
    "\n",
    "#     num_roi = bboxes.shape[0]\n",
    "\n",
    "#     input_images = torch.cat([images, images], axis=0)\n",
    "#     input_labels = torch.cat([labels, fake_labels], axis=0)\n",
    "#     input_bboxes = torch.cat([bboxes, bboxes], axis=0)\n",
    "\n",
    "#     input_bboxes[num_roi:, 0] += batch_size  # increment image id\n",
    "#     input_bboxes[:, 1:] = input_bboxes[:, 1:] / 2  # correct for pooling\n",
    "\n",
    "#     valid = torch.ones((num_roi, 1))\n",
    "#     fake = torch.zeros((num_roi, 1))\n",
    "\n",
    "#     targets = torch.cat([valid, fake], axis=0).to(device)\n",
    "\n",
    "#     # clear previous gradients\n",
    "#     optimiser.zero_grad()\n",
    "\n",
    "#     # forward pass\n",
    "#     validity = roi_gan(input_images, input_labels, input_bboxes)\n",
    "\n",
    "#     # calculate loss\n",
    "#     d_loss = MSELoss()(validity, targets)\n",
    "\n",
    "#     # backpropagate\n",
    "#     d_loss.backward()\n",
    "\n",
    "#     # descent step\n",
    "#     optimiser.step()\n",
    "\n",
    "#     return d_loss\n",
    "\n",
    "\n",
    "# def train_on_batch_combined(models, data, optimiser):\n",
    "\n",
    "#     # clear previous gradients\n",
    "#     optimiser.zero_grad()\n",
    "\n",
    "#     generator, discriminator, roi_gan = models\n",
    "#     images, labels, bboxes = data\n",
    "#     bboxes[:, 1:] = bboxes[:, 1:] / 2  # correct for pooling\n",
    "\n",
    "#     fake_labels = generator(images)\n",
    "\n",
    "#     # Discriminators determines validity of translated images\n",
    "#     num_roi = bboxes.shape[0]\n",
    "#     valid_roi = torch.ones((num_roi, 1)).to(device)\n",
    "#     validity_roi = roi_gan(images, fake_labels, bboxes)\n",
    "\n",
    "#     # Discriminators determines validity of translated images\n",
    "#     valid_patch = torch.ones((batch_size,) + disc_patch).to(device)\n",
    "#     validity_patch = discriminator(images, fake_labels)\n",
    "\n",
    "#     # Best results with (0.2, 1, 5)\n",
    "\n",
    "#     g_loss = 0.2 * MSELoss()(validity_patch, valid_patch) + \\\n",
    "#              1 * MSELoss()(validity_roi, valid_roi) + \\\n",
    "#              5 * L1Loss()(labels, fake_labels)\n",
    "\n",
    "#     # calculate gradients\n",
    "#     g_loss.backward()\n",
    "\n",
    "#     # descent step\n",
    "#     optimiser.step()\n",
    "\n",
    "#     return g_loss\n",
    "\n",
    "\n",
    "loss_record = torch.Tensor()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_i in range(steps_per_epoch):\n",
    "\n",
    "        data = next(train_gen)\n",
    "        images, labels, bboxes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "\n",
    "        # Generate fake images\n",
    "        fake_labels = generator(images).detach()\n",
    "\n",
    "        # Train discriminator\n",
    "        set_trainable(discriminator, True)\n",
    "\n",
    "        d_loss = torch.sqrt(train_on_batch_discriminator(discriminator,\n",
    "                            [images, labels, fake_labels],\n",
    "                            optimiser_discriminator, device)).item()\n",
    "\n",
    "        set_trainable(discriminator, False)\n",
    "\n",
    "        # Train roi discriminator\n",
    "        set_trainable(roi_gan, True)\n",
    "\n",
    "        roi_loss = torch.sqrt(train_on_batch_roi_gan(roi_gan,\n",
    "                              [images, labels, fake_labels, bboxes],\n",
    "                              optimiser_roi, device)).item()\n",
    "\n",
    "        set_trainable(roi_gan, False)\n",
    "\n",
    "        # Train combined pix2pix\n",
    "        g_loss = train_on_batch_combined([generator, discriminator, roi_gan],\n",
    "                                         [images, labels, bboxes],\n",
    "                                         optimiser_combined, lambdas=(0.2, 1, 5), device).item()\n",
    "\n",
    "        loss_record = torch.cat([loss_record, torch.Tensor([[d_loss, roi_loss, g_loss]])])\n",
    "\n",
    "        # Print losses\n",
    "        if batch_i % 100 == 0:\n",
    "            print('[Epoch %d/%d] [Batch %03d/%d] [D loss: %f] [R loss: %f] [G loss: %05f]' % (\n",
    "                epoch, epochs, batch_i, steps_per_epoch, d_loss, roi_loss, g_loss))\n",
    "\n",
    "    # save model weights\n",
    "    torch.save(generator.state_dict(), './pix2pix_generator_%d.torch' % epoch)\n",
    "    torch.save(discriminator.state_dict(), './pix2pix_discriminator_%d.torch' % epoch)\n",
    "    torch.save(roi_gan.state_dict(), './pix2pix_discriminator_%d.torch' % epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 199 \n",
    "generator = FNet().to(device)\n",
    "generator.load_state_dict(torch.load('./weights//pix2pix_generator_%d.torch' % epoch,\n",
    "                                     map_location=torch.device('cpu')))\n",
    "generator = generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise progress\n",
    "gen_test = gen_canvas(x_test, y_test, batch_size=3)\n",
    "data = next(gen_test)\n",
    "images, labels, bboxes = data[0].to(device), data[1].to(device), data[2].to(device)\n",
    "\n",
    "fake_labels = generator(images)\n",
    "\n",
    "titles = ['Original', 'Generated']\n",
    "fig, axes = plt.subplots(figsize=(10, 15), nrows=3, ncols=2)\n",
    "\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 1].set_title('Generated')\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    axes[i, 0].imshow(labels.detach().cpu().numpy()[i].squeeze())\n",
    "    axes[i, 1].imshow(fake_labels.detach().cpu().numpy()[i].squeeze())\n",
    "\n",
    "# fig.savefig('./outputs/torch_%04d.png' % epoch)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
